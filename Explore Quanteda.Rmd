---
title: "Textual Analysis with Quanteda"
author: "Zhaofeng Tong"
output: html_document
---



1.  Getting Started.

    Install Packages 
    ```{r, eval=FALSE}
    install.packages("quanteda")
    install.packages("readtext")
    ```
    

    ```{r, eval=FALSE}
    devtools::install_github('quanteda/quanteda.corpora')
    ```
    
    
1.  Exploring **quanteda** functions.
    
    ```{r, echo = T, message=FALSE}
 
    library(tm)
    library(quanteda)
    library(quanteda.corpora)
    library(readtext)

    ```

   



2.  Making a corpus and corpus structure

    

```{r,eval=FALSE}
a1<-corpus(data_char_ukimmig2010)
a2<-corpus(data_corpus_inaugural)
summary(a1)
summary(a2)
```
      
   
```{r}
library("quanteda", quietly = TRUE, warn.conflicts = FALSE)
```


   

 
3.  Explore some phrases in the text.  

 
      
    ```{r}
kwic(data_corpus_inaugural, "terror",3)
    ```

    

4.  Create a document-feature matrix.
   
   
```{r}
mydfm <- dfm(data_corpus_inaugural, remove = stopwords("english"))
mydfm
topfeatures(mydfm, 20)
```
   
    
Group by president  
```{r}
mydfm <- dfm(data_corpus_inaugural, groups = "President")
mydfm
docnames(mydfm)


```
    

```{r}
mydfm1<-dfm(data_corpus_irishbudget2010,groups = "party")
docvars(mydfm1)

```
5.  Explore the ability to subset a corpus.  

 
   
```{r}
    obamadfm <- dfm(corpus_subset(data_corpus_inaugural, President=="Obama"))
    textplot_wordcloud(obamadfm)
```


```{r}
am<-dfm_remove(obamadfm,stopwords("english"))
textplot_wordcloud(am)
```
1.  **Preparing and pre-processing texts**


```{r}
data("data_char_sampletext")
char_tolower(data_char_sampletext,acronyms=TRUE)

```

```{r}
?tokens()
e<-tokens(data_char_sampletext,remove_symbols=TRUE,remove_punct=TRUE)
```
6.  Tokenizing texts

7.  Stemming.
```{r}

library(SnowballC)

char_wordstem(as.character(e),language = quanteda_options("language_stemmer"))
```


8.  Applying "pre-processing" to the creation of a `dfm`.

        
```{r}
        require(tm)
        data("crude")
        crude <- tm_map(crude, content_transformer(tolower))
        crude <- tm_map(crude, removePunctuation)
        crude <- tm_map(crude, removeNumbers)
        crude <- tm_map(crude, stemDocument)
        tdm <- TermDocumentMatrix(crude)

        # same in quanteda
        require(quanteda)
        crudeCorpus <- corpus(crude)
        crudeDfm <- dfm(crudeCorpus, remove_punct = T, remove_numbers = T, stem = T)
        docnames(crudeDfm)
        topfeatures(crudeDfm)
        dim(crudeDfm)
        
```        



This indicates that we can extract the names of the words from the **tm** TermDocumentMatrix object by getting the rownames from inspecting the tdm:
```{r}
head(tdm$dimnames$Terms, 848)
```
Compare this to the results of the same operations from **quanteda**.  To get the "words" from a quanteda object, you can use the `featnames()` function:
```{r}
features_quanteda <- featnames(crudeDfm) #874 features
head(features_quanteda, 20)
str(crudeDfm)
```        
What proportion of the `crudeDfm` are zeros?  Compare the sizes of `tdm` and `crudeDfm` using the `object.size()` function.
        
```{r}

prop.table(table(as.matrix(crudeDfm)==0))

print(object.size(crudeDfm), units= "Mb")

print(object.size(tdm), units= "Mb")
```

1.  **Keywords-in-context**




```{r}
a7<-corpus(data_corpus_irishbudget2010)
mykwic<-kwic(a7,"good",window = 3)
str(mykwic)
```



```{r}
?kwic
mykwic2<-kwic(a7,"disaster",window = 0)

```

1.  **Descriptive statistics**
    
    1.  We can extract basic descriptive statistics from a corpus from
        its document feature matrix.  Make a dfm from the 2010 Irish budget 
        speeches corpus.
```{r}
dfmm<-dfm(data_corpus_irishbudget2010)
?textstat_frequency
textstat_frequency(dfmm,n=5)
```


```{r}
        # count syllables from texts in the 2010 speech corpus 
        textsyls <- nsyllable(texts(data_corpus_irishbudget2010))
        # sum the syllable counts 
        sum(textsyls)                           
```
        
        How would you get the total syllables per text?
        
3.  **Lexical Diversity over Time**



```{r}
        data(data_corpus_irishbudgets, package = "quanteda.corpora")
        finMins <- corpus_subset(data_corpus_irishbudgets, number == "01")
        ?corpus_subset
        tokeninfo <- summary(finMins)
```
        

```{r,eval=FALSE,error=FALSE}
q<-textstat_lexdiv(dfmm,measure="TTR")
str(q)
plot(q$TTR,data_corpus_irishbudget2010$year,type = "l")
summary(corpus(data_corpus_irishbudget2010))
plot(textstat_lexdiv(dfmm,measure = "TTR"))
```


4.  **Document and word associations**

    1.  Load the presidential inauguration corpus selecting from 1900-1950,
        and create a dfm from this corpus.
```{r}
dfm1 <- dfm(corpus_subset(data_corpus_inaugural,Year>1900 & Year<1950))

```
    2.  Measure the term similarities (`textstat_simil`) for the following words: *economy*, *health*,
        *women*. Which other terms are most associated with each of these three terms?
```{r}
?textstat_simil
textstat_simil(dfm1,selection = c("economy","health"),margin="features")
```
1.  **Working with dictionaries**

```{r}
  posDict <- dictionary(list(articles = c("the", "a", "and"),
                                   conjunctions = c("and", "but", "or", "nor", "for", "yet", "so")))
```
        

```{r}
        posDfm <- dfm(data_corpus_inaugural, dictionary = posDict)
        posDfm[1:10,]
```

